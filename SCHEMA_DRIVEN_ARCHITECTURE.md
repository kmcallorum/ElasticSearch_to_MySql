# ðŸŽ¯ SCHEMA-DRIVEN ARCHITECTURE - PROJECT-AGNOSTIC PIPELINE

## ðŸ’¡ THE PROBLEM YOU IDENTIFIED:

âœ… **Elasticsearch structure varies per project** at Optum  
âœ… **Your current project** uses Fortify scan schema  
âœ… **Other teams** have totally different schemas  
âœ… **Pipeline should be generic** - not tied to any specific structure  
âœ… **Need a way to say:** "Here's MY schema for MY project"  
âœ… **Auto-generate test data** from that schema  

---

## âœ… THE SOLUTION: CONFIGURATION-DRIVEN DESIGN

### **Project Structure:**

```
es-to-mysql-pipeline/
â”‚
â”œâ”€â”€ schemas/                     # Schema definitions (per-project)
â”‚   â”œâ”€â”€ fortify_scans.json      # Your current Fortify project
â”‚   â”œâ”€â”€ jenkins_builds.json     # Another team's project
â”‚   â”œâ”€â”€ deployment_logs.json    # Another team's project
â”‚   â””â”€â”€ sonarqube_scans.json    # Another team's project
â”‚
â”œâ”€â”€ generate_test_data.py        # Generic test data generator
â”‚
â”œâ”€â”€ pipeline_cli.py               # Generic pipeline (schema-agnostic!)
â”‚
â””â”€â”€ README.md                     # How to add your schema
```

---

## ðŸ“ HOW IT WORKS:

### **Step 1: Define YOUR Schema**

Create `schemas/fortify_scans.json`:

```json
{
  "name": "Fortify Security Scans",
  "description": "Fortify scan results from Jenkins at Optum",
  
  "elasticsearch": {
    "index_pattern": "fortify-scans-*",
    "id_field": "_id"
  },
  
  "sample_document": {
    "@version": "1",
    "eventData": {...},
    "fortifyData": {...}
  },
  
  "variation_rules": {
    "eventData.status": {
      "type": "random_choice",
      "values": ["success", "failure"],
      "weights": [0.9, 0.1]
    },
    "fortifyData.fortifyBuildName": {
      "type": "increment",
      "start": 46089
    }
  }
}
```

---

### **Step 2: Generate Test Data**

```bash
# List available schemas
python generate_test_data.py --list

# Generate 100 test records for YOUR schema
python generate_test_data.py \
  --schema fortify_scans \
  --count 100 \
  --output test_data.jsonl
```

**Output:**
```
ðŸš€ Generating test data...
   Schema: fortify_scans
   Count: 100
   Format: jsonl
   
âœ… Generated 100 records
âœ… Saved to: test_data.jsonl
```

---

### **Step 3: Run Generic Pipeline**

```bash
# Pipeline doesn't care about your schema!
python pipeline_cli.py \
  --source_type jsonl \
  --input_file test_data.jsonl \
  --sink_type mysql \
  --db_host localhost \
  --db_user root \
  --db_pass password \
  --db_name fortify \
  --db_table scans \
  --threads 10 \
  --metrics-port 8000
```

---

## ðŸ”¥ KEY FEATURES:

### **1. Schema-Agnostic Pipeline**

The pipeline **doesn't know or care** about your data structure:
- âœ… Works with ANY JSON schema
- âœ… No hardcoded field names
- âœ… No project-specific logic
- âœ… Pure data movement

### **2. Per-Project Schemas**

Each team defines their own:
```
schemas/
â”œâ”€â”€ fortify_scans.json          â† Your Fortify project
â”œâ”€â”€ jenkins_builds.json         â† DevOps team's schema
â”œâ”€â”€ deployment_logs.json        â† SRE team's schema
â””â”€â”€ sonarqube_results.json      â† QA team's schema
```

### **3. Realistic Test Data**

The generator creates **realistic variations**:

```json
{
  "variation_rules": {
    "eventData.status": {
      "type": "random_choice",
      "values": ["success", "failure"],
      "weights": [0.9, 0.1]  // 90% success, 10% failure
    },
    "eventData.duration_ms": {
      "type": "random_int",
      "min": 0,
      "max": 1000
    },
    "fortifyData.fortifyBuildName": {
      "type": "increment",  // Auto-increment
      "start": 46089
    },
    "pipelineData.gitCommit": {
      "type": "random_hex",  // 40-char hex string
      "length": 40
    }
  }
}
```

---

## ðŸŽ¯ USAGE EXAMPLES:

### **Your Current Fortify Project:**

```bash
# 1. Already have your schema: schemas/fortify_scans.json

# 2. Generate test data
python generate_test_data.py --schema fortify_scans --count 1000

# 3. Test the pipeline
python pipeline_cli.py \
  --source_type jsonl \
  --input_file test_data_fortify_scans.jsonl \
  --sink_type mysql \
  --db_host your-mysql \
  --db_name fortify \
  --db_table scans \
  --metrics-port 8000

# 4. In production, use real Elasticsearch
python pipeline_cli.py \
  --source_type elasticsearch \
  --es_host your-es-cluster \
  --es_index fortify-scans-* \
  --sink_type mysql \
  ...
```

### **Another Team's Jenkins Project:**

```bash
# 1. Create schemas/jenkins_builds.json (their schema)

# 2. Generate their test data
python generate_test_data.py --schema jenkins_builds --count 500

# 3. Same pipeline, different data!
python pipeline_cli.py \
  --source_type jsonl \
  --input_file test_data_jenkins_builds.jsonl \
  --sink_type mysql \
  --db_name jenkins \
  --db_table builds
```

---

## ðŸ“Š VARIATION RULE TYPES:

### **Random Values:**
```json
{"type": "random_int", "min": 0, "max": 1000}
{"type": "random_float", "min": 0.0, "max": 1.0}
{"type": "random_choice", "values": ["a", "b", "c"]}
{"type": "random_choice", "values": ["success", "failure"], "weights": [0.9, 0.1]}
{"type": "random_hex", "length": 40}
{"type": "random_string", "length": 20}
```

### **Sequential Values:**
```json
{"type": "increment", "start": 1000, "step": 1}
{"type": "timestamp_increment", "start": 1609840946994, "step_ms": 1000}
```

### **Fixed Values:**
```json
{"type": "constant", "value": "always-this-value"}
```

---

## ðŸš€ HOW TO ADD YOUR SCHEMA:

### **1. Create Schema File:**

`schemas/your_project.json`:

```json
{
  "name": "Your Project Name",
  "description": "What this data represents",
  
  "elasticsearch": {
    "index_pattern": "your-index-*",
    "id_field": "_id"
  },
  
  "sample_document": {
    // Paste ONE real document from your ES
    "field1": "value1",
    "nested": {
      "field2": "value2"
    }
  },
  
  "variation_rules": {
    // Which fields should vary in test data
    "field1": {"type": "random_choice", "values": ["a", "b"]},
    "nested.field2": {"type": "increment", "start": 1}
  }
}
```

### **2. Generate Test Data:**

```bash
python generate_test_data.py --schema your_project --count 100
```

### **3. Test Pipeline:**

```bash
python pipeline_cli.py \
  --source_type jsonl \
  --input_file test_data_your_project.jsonl \
  --sink_type mysql \
  ...
```

### **4. Production:**

```bash
python pipeline_cli.py \
  --source_type elasticsearch \
  --es_host your-real-es \
  --es_index your-real-index \
  --sink_type mysql \
  ...
```

---

## ðŸ’ª WHY THIS IS AWESOME:

### **For You (Fortify Project):**
âœ… Define your Fortify schema once  
âœ… Generate realistic test data instantly  
âœ… Test pipeline with your exact structure  
âœ… No hardcoded Fortify logic in pipeline  

### **For Other Teams:**
âœ… They define THEIR schema  
âœ… Generate THEIR test data  
âœ… Use THE SAME PIPELINE  
âœ… No code changes needed  

### **For the Organization:**
âœ… One pipeline, many projects  
âœ… Configuration-driven  
âœ… Easy to onboard new teams  
âœ… Consistent approach across Optum  

---

## ðŸŽ¤ THE PITCH TO DAN:

> *"Built a schema-driven pipeline that works with ANY Elasticsearch structure.*
> 
> *Each team defines their schema in a config file.*  
> *Generator creates realistic test data from that schema.*  
> *Pipeline is completely generic - no project-specific code.*
> 
> *Your Fortify scans? One config file.*  
> *DevOps Jenkins builds? Different config file.*  
> *Same pipeline. Same tests. Same metrics.*
> 
> *100% test coverage. Production-ready. Reusable across Optum."*

---

## ðŸ“¦ FILES PROVIDED:

1. âœ… **generate_test_data.py** - Schema-driven test data generator
2. âœ… **schemas/fortify_scans.json** - Your Fortify schema
3. âœ… **README_SCHEMAS.md** - How to add schemas (this file)

---

## ðŸ”„ WORKFLOW:

```
New Project at Optum
        â†“
Create schema file (5 min)
        â†“
Generate test data (30 sec)
        â†“
Test pipeline (1 min)
        â†“
Production ready! âœ…
```

---

## âœ¨ NEXT STEPS:

```bash
# 1. Create schemas directory
mkdir schemas

# 2. Save your Fortify schema
cp fortify_scans.json schemas/

# 3. List schemas
python generate_test_data.py --list

# 4. Generate test data
python generate_test_data.py --schema fortify_scans --count 100

# 5. Test pipeline
python pipeline_cli.py \
  --source_type jsonl \
  --input_file test_data_fortify_scans.jsonl \
  --sink_type jsonl \
  --output_file output.jsonl \
  --metrics-port 8000

# 6. Verify output
cat output.jsonl | jq .
```

---

**This is how you make it project-agnostic while keeping it specific to YOUR needs!** ðŸŽ¯

Each team gets their own schema config. Same pipeline. Same quality. Same metrics. âœ…
