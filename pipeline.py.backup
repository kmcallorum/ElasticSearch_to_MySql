"""
Data pipeline with dependency injection - processes records from source to sink

Author: Kevin McAllorum (kevin_mcallorum@linux.com)
GitHub: github.com/kmcallorum
License: MIT
"""
import logging
import threading
from queue import Queue
from typing import Optional, Dict, Any
from data_interfaces import DataSource, DataSink
from error_analyzer import ErrorAnalyzer, NoOpErrorAnalyzer

logger = logging.getLogger(__name__)


class DataPipeline:
    """
    Main pipeline that orchestrates data movement from source to sink.
    Uses dependency injection for testability.
    """
    
    def __init__(self, source: DataSource, sink: DataSink, num_threads: int = 5,
                 error_analyzer: Optional[ErrorAnalyzer] = None):
        """
        Args:
            source: DataSource implementation
            sink: DataSink implementation  
            num_threads: Number of worker threads for parallel processing
            error_analyzer: Optional ErrorAnalyzer for AI-powered troubleshooting
        """
        self.source = source
        self.sink = sink
        self.num_threads = num_threads
        self.error_analyzer = error_analyzer or NoOpErrorAnalyzer()
        self.total_processed = 0
    
    def run(self, query_params: Optional[Dict[str, Any]] = None):
        """
        Execute the pipeline: fetch from source and insert into sink.
        
        Args:
            query_params: Optional parameters to pass to the data source
        """
        logger.info(f"Starting pipeline with {self.num_threads} threads")
        
        # Single-threaded for sinks that aren't thread-safe (like file writes)
        if self.num_threads == 1:
            self._run_single_threaded(query_params)
        else:
            self._run_multi_threaded(query_params)
        
        # Final commit and stats
        self.sink.commit()
        stats = self.sink.get_stats()
        logger.info(f"Pipeline completed. Total processed: {self.total_processed}")
        logger.info(f"Final stats: {stats}")
        
        return stats
    
    def _run_single_threaded(self, query_params: Optional[Dict[str, Any]]):
        """Single-threaded execution (safer for file-based sinks)"""
        try:
            for record_id, content in self.source.fetch_records(query_params):
                try:
                    self.sink.insert_record(record_id, content)
                    self.total_processed += 1
                    
                    if self.total_processed % 100 == 0:
                        logger.info(f"Processed {self.total_processed} records")
                except Exception as e:
                    self._handle_error(e, {
                        "operation": "sink_insert",
                        "record_id": record_id,
                        "total_processed": self.total_processed
                    })
                    # Continue processing other records
        except Exception as e:
            self._handle_error(e, {
                "operation": "source_fetch",
                "total_processed": self.total_processed
            })
            raise  # Re-raise source errors as they're fatal
    
    def _run_multi_threaded(self, query_params: Optional[Dict[str, Any]]):
        """Multi-threaded execution (for thread-safe sinks like MySQL)"""
        queue = Queue()
        threads = []
        
        # Start worker threads
        for i in range(self.num_threads):
            t = threading.Thread(
                target=self._insert_worker,
                args=(queue,),
                name=f"Worker-{i+1}"
            )
            t.start()
            threads.append(t)
        
        # Feed queue from source
        for record_id, content in self.source.fetch_records(query_params):
            queue.put((record_id, content))
            self.total_processed += 1
            
            if self.total_processed % 100 == 0:
                logger.info(f"Queued {self.total_processed} records")
        
        # Wait for queue to empty and stop workers
        queue.join()
        for _ in threads:
            queue.put(None)  # Poison pill
        for t in threads:
            t.join()
    
    def _insert_worker(self, queue: Queue):
        """Worker thread that processes items from queue"""
        worker_stats = {"processed": 0, "inserted": 0, "skipped": 0}
        
        while True:
            item = queue.get()
            if item is None:  # Poison pill
                break
            
            record_id, content = item
            inserted = self.sink.insert_record(record_id, content)
            
            worker_stats["processed"] += 1
            if inserted:
                worker_stats["inserted"] += 1
            else:
                worker_stats["skipped"] += 1
            
            if worker_stats["processed"] % 100 == 0:
                logger.debug(f"{threading.current_thread().name} - {worker_stats}")
            
            queue.task_done()
        
        logger.info(f"{threading.current_thread().name} finished: {worker_stats}")
    
    def cleanup(self):
        """Clean up resources"""
        self.source.close()
        self.sink.close()
    
    def _handle_error(self, error: Exception, context: Dict[str, Any]):
        """
        Handle an error with optional AI analysis
        
        Args:
            error: The exception that occurred
            context: Context about where/when the error occurred
        """
        # Log the error
        logger.error(f"Error in {context.get('operation', 'unknown')}: {error}")
        
        # Get AI suggestions if analyzer is enabled
        if self.error_analyzer.is_enabled():
            try:
                suggestions = self.error_analyzer.analyze_error(error, context)
                if suggestions:
                    logger.info(f"\n{suggestions}\n")
            except Exception as analysis_error:
                logger.debug(f"Error analysis failed (non-critical): {analysis_error}")
